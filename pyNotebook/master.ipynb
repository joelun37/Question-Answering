{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract / Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's not Google Knowledge Graph.\n",
    "\n",
    "Massive collection of full-text documents today. It used to be enough to just search and send the list of documents containinf the answer but nowadays there are billions of files and thousands probably contain your answer. Can we rather send the answer to the question? Especially for mobile phone use.\n",
    "\n",
    "Two parts: \n",
    "\n",
    "1. Still do information retreivel techniques, to find out which documents contain the answers. Traditional techniques are scalable over billions of documents. Whereas, current systems are not. But once we got our candidate, we'd like to search for the answer.\n",
    "\n",
    "2. Findding the answer in the document: reading comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading comprehension\n",
    "\n",
    "Started back in the 70s.\n",
    "\n",
    "A brief history of Reading Comprehension:\n",
    "\n",
    "- Schank, Abelson, Lehnert et al. c. 1977 - \"Yale AI Project\", nothing much came out of that.\n",
    "\n",
    "- Revived by Lynette Hirschman in 1999: could NLP systems answer human reading comprehension questions from 3rd to 6th graders? Using simple methods.\n",
    "\n",
    "\n",
    "- Revived again by Chris Burges in 2013 with MCTest: answering questions over simple stroy texts\n",
    "\n",
    "Burges : \"A machine comprehends a passage of text if, for any question regarding the text thatcan be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question, and does not contain information irrelevant to that question.\" from Towards the Machine Comprehension of Text: An Essay, December 23, 2013.\n",
    "\n",
    "- Floodgates opened in 2015/16 with the production of *large datasets* which permit supervised neural systems to be built: Hemann et al. (NIPS 2015), Rajpurkar et al. (EMNLP 2016 - SQuAD), MS MARCO, TriviaQA, RACE, NewsQA, NarrativeQA, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-domain Question Answering\n",
    "\n",
    "- Simmons et al. (1964) did first exploration of answering questions from an expository text based on matching dependancy parses of a question and answer\n",
    "\n",
    "- Murax (Kupiec 1993) aimed to answer questions over an online encyclopedia using IR and shallow linguistic processing\n",
    "\n",
    "- The NIST TREC QA track begun in 1999 first rigourously investigated answering fact questions over a large collection of documents\n",
    "\n",
    "- IBM's Jeopardy System (DeepQA, 2011) brought attention to a version of the problem; it used an ensemble of many methods\n",
    "\n",
    "- DrQA (Chen et al. 2016) uses IR followed by neural reading comprehension to bring deep learning to Open-domain QA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Talk about what models you've built for awhile.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor in the video mention data after the models.\\\n",
    "\"Discussion about what data you're using for your projects. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Figures and tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"More tables and figures\". Results showing how your system works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Error analysis to see what you got right and wrong\"\n",
    "\"Plans for the future and conclusions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
