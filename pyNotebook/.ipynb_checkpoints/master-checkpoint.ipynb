{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to review different methods of Question Answering in the machine learning literature by applying them to a dataset that was made available by Google for the Kaggle challenge *TensorFlow 2.0 Question Answering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Literature\n",
    "\n",
    "In the past, it was enough to just search and send the list of documents containing an information. However, today, massive collections of full-text documents are available on-line. Among those billions of files, thousands probably contain the answer to a question. The aim of Question Answering is using machine learning techniques to sieve through those documents and find the answer that will meet the user's needs.\n",
    "\n",
    "In this section, we present Question Answering literature and popular pre-trained datasets such as SQuAD and BERT.\n",
    "\n",
    "## Reading comprehension\n",
    "\n",
    "Question Answering literature first started with the Reading Comprehension problem back in the 70s. Professor Robert Schank launched the **Yale AI Project** around 1977 with Robert Abelson and Wendy Lehnert. These research efforts were pretty much fruitless until late 90s; when Lynette Hirschman et al. published **Deep Read: A Reading Comprehension System** in 1999. In that paper, authors used simple NLP methods such as the *Bag-of-words* approach to answer questions from 3rd to 6th grade material. The literature was revived again by Chris Burges et al. in 2013 with the paper **MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text**. In that paper, they created a Question Answering model that would find answers to questions over simple story texts crowdsourced on *Amazon Mechanical Turk*. In his publication for Microsoft Research, **Towards the Machine Comprehension of Text: An Essay**, Burges states \"A machine comprehends a passage of text if, for any question regarding the text thatcan be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question, and does not contain information irrelevant to that question.\".\n",
    "\n",
    "- Floodgates opened in 2015/16 with the production of *large datasets* which permit supervised neural systems to be built: Hemann et al. (NIPS 2015), Rajpurkar et al. (EMNLP 2016 - SQuAD), MS MARCO, TriviaQA, RACE, NewsQA, NarrativeQA, ...\n",
    "\n",
    "## Open-domain Question Answering\n",
    "\n",
    "- Simmons et al. (1964) did first exploration of answering questions from an expository text based on matching dependancy parses of a question and answer\n",
    "\n",
    "- Murax (Kupiec 1993) aimed to answer questions over an online encyclopedia using IR and shallow linguistic processing\n",
    "\n",
    "- The NIST TREC QA track begun in 1999 first rigourously investigated answering fact questions over a large collection of documents\n",
    "\n",
    "- IBM's Jeopardy System (DeepQA, 2011) brought attention to a version of the problem; it used an ensemble of many methods\n",
    "\n",
    "- DrQA (Chen et al. 2016) uses IR followed by neural reading comprehension to bring deep learning to Open-domain QA.\n",
    "\n",
    "## Turn-of-the Millenium Full NLP QA:\n",
    "\n",
    "- Complex systems but the did work fairly well on \"factoid\" questions, e.g. architecture of LCC (Harabagio/Moldovan) QA system, circa 2033\n",
    "\n",
    "Factoid QA, people in NLP used it. e.g. What year was Elvis Presley born, what is the name of Beyonce's husband, etc. Anything for which there's a clear answer. Like most of web searches, these systems actually did work well in the past, up to 70%. However, it stayed there. These were very complex systems, with a huge amount of hand-built stuff. \n",
    "\n",
    "Contrast between this and Deep Learning.\n",
    "\n",
    "## Stanford Question Answering Dataset (SQuAD)\n",
    "\n",
    "By construction for SQuAD an answer is always a sub-sequence of words in a passage (a span). \n",
    "\n",
    "100k examples. Answer must be a span in the passage. aka extractive question answering\n",
    "\n",
    "### SQuAD v1.1.\n",
    "\n",
    "How it was built: They used Mechanical Turk to find three people to read and give answers--> three answers per question.\n",
    "\n",
    "Examples, given a paragraph:\n",
    "\n",
    "- Along with non-governmental and non-state schools, what is anbother name for schools? A1: indepedent, A2: independent schools, A3: independent schools\n",
    "\n",
    "- Along with sport and art, what is a type of talent scholarship? A1: academic, A2: academic, A3: academic\n",
    "\n",
    "Evaluation metrics:\n",
    "\n",
    "1. Exact match (1/0)\n",
    "2. F1: Take system and each gold answer as bag of words, evaluate macro-average of per-question F1 scores.\n",
    "\n",
    "F1 explanation: you treat the system span on each gold answer as a bag of worlds.\n",
    "\n",
    "P = the percent of words in the system's answer that are actually in a gold span\n",
    "R = the percent of words in a gold span that are in the system's span\n",
    "\n",
    "Calculate the harmonic mean of those two numbers, harmonic mean being a very conservative average so it's close to MIN of those two numbers. For each question then the F1 is the maximum F1 score over three different answers, then average the F1 of questions.\n",
    "\n",
    "Why not use exact match? \n",
    "\n",
    "Even if it gives a little bit of robustness, three people is not a large sample. So statistically not meaningful.\n",
    "\n",
    "Both metrics ignore punctuation and \"a\" and \"the\".\n",
    "\n",
    "Humans had F1=91.2\n",
    "\n",
    "Logistic regression baseline, F1 = 51.0.\n",
    "\n",
    "BERT single = 91.835 - Google AI Language\n",
    "BERT ensemble = 93.160\n",
    "nlnet ensemble = 91.202 - Microsoft Research Asia\n",
    "QANet ensemble = 90.490 - Google Brain and CMU\n",
    "r-net ensemble = 90.147 - Microsoft Research Asis\n",
    "\n",
    "### SQuAD 2.0\n",
    "\n",
    "- A defect of SQuAB 1.0 is that all questions have an answer in the paragraph\n",
    "\n",
    "- Systems (implicitly) rank candidates and choose the best one\n",
    "\n",
    "- You don't have to judge whether a span answers the question\n",
    "\n",
    "- In SQuAD 2.0, 1/3 of the training questions have no answer, and about 1/2 of the dev/test questions have no answer. For NoAnswer examples, NoAnswer receives a score of 1, and any other response gets 0, for both exact match and F1\n",
    "\n",
    "- Simplest system approach to SQuAD 2.0: Have a threshold score for whether a span answers a question\n",
    "\n",
    "- Or you could have a second component that confirms answering such as Natural Language Inference (NLI) or \"Answer validation\"\n",
    "\n",
    "**SQuAD limitations:**\n",
    "\n",
    "- SQuAD has a number of other key limitations too: (i) only span-based answers (yes/no, counting, implicit why), (ii) questions were constructed looking at the passages, and (iii) barely any multi-fact/sentence inference beyond coreference\n",
    "\n",
    "Nevertheless, it is well-targeted, well-structured, clean dataset\n",
    "\n",
    "- it thas been the most used and competed on QA dataset\n",
    "- it has also been a useful starting point for building systems in industry (though in-domain data always really helps!)\n",
    "\n",
    "## BERT\n",
    "\n",
    "What is BERT, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Stanford Attentive Reader\n",
    "\n",
    "\"Talk about what models you've built for awhile.\"\n",
    "\n",
    "Use **Stanford Attentive Reader** as your baseline.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "The way it works is; first we have a question like \"Which team won SuperBowl L?\"\n",
    "\n",
    "1. Buil a representation of a question as a vector.\n",
    "2. For each word in question, we look up a word embedding (binary)\n",
    "3. Then run an LSTM forward and backward. \n",
    "4. Grab the end state of LSTMs and simply concatenate them together into a vector of dimension 2D if our hidden states of LSTM are D and we say that is the representation of the question.\n",
    "5. Then start looking at the passage. For the start of dealin with the passage, we do the same thing. That is, we look up a word vector for every word in the passage and we run a bidirectional LSTM, now being represented a bit more compactly across the passage.\n",
    "6. Use the question representation to work out where the answer is using attention. This is a different use of attention to machine translation. The kind of attention equations are still exactly the same. But we've now got this one question vector that we're going to be trying to match against to return the answer. Work out an attention score between each word's bi-LSTM representation and the question. We're using the following bi-linear attention \n",
    "\n",
    "$$\\alpha_{i} = \\text{softmax}_{i}(\\mathbf{q}^\\top\\mathbf{W}_{s}\\tilde{\\mathbf{p}}_i) $$\n",
    "\n",
    "where $\\mathbf{q}$ is the question vector, $ \\tilde{\\mathbf{p}} $ is the vector for a particular position in the passage, the two concatenated LSTM hidden states, $ \\mathbf{W} $ is the learning matrix.\n",
    "\n",
    "We calculate that quantity for each position and then we put through a softmax which will give us probability over the different words in the passage. And those give us attention weights. So at that point, we have attention weights for different positions in the passafe and we just declare that that is where the answer starts.\n",
    "\n",
    "7. To get the end of the answer, we do exactly the same thing again apart from we train a different $ \\mathbf{W} $ matrix.\n",
    "\n",
    "What if the most important words are in the middle of the answer? It's the bi-LSTM's job to push the limits to the extremes of the span, so that this simple bi-linear attention will get a big score at the start of the span. \n",
    "\n",
    "How come we use exactly the same equation to detect the beginning and the end of the answer? How come one of them is meant to know it's picking up beginning and the other the end? We just tell the neural network to learn a matrix for the beginning and a different one for the end. That constraint, will pressure the neural network to self orgnize itself in such a way that there will be some parts of this hidden representation that will be good at learning starts of spans, and some parts that will where the span's end is. The $ \\mathbf{W} $ matrix will be able to pick out those parts of the representation.  \n",
    "\n",
    "This very simple model works very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Stanford Attentive Reader++\n",
    "\n",
    "\n",
    "https://github.com/peterkim95/attentive-reader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: BERT?\n",
    "\n",
    "http://mccormickml.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor in the video mention data after the models.\\\n",
    "\"Discussion about what data you're using for your projects. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Figures and tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"More tables and figures\". Results showing how your system works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Error analysis to see what you got right and wrong\"\n",
    "\"Plans for the future and conclusions\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
