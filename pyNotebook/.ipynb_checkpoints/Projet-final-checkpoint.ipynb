{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Natural Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Le but de ce challenge est de créer un modèle d'apprentissage statistique pour répondre aux questions posées en se servant d'un corps de texte (i.e. Wikipedia). Pour accéder au lien du *challenge*, veuillez clicquez [ici](https://ai.google.com/research/NaturalQuestions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation des données\n",
    "\n",
    "Dans cette partie nous présentons les données fournies. Le format des données d'apprentissage est `json`. En revanche Google a créé une version tabulée des données qui se trouve dans ce [lien](https://ai.google.com/research/NaturalQuestions/databrowser).\n",
    "\n",
    "Pour visualiser une observation de la base simplifiée, veuillez cliquez [ici](https://raw.githubusercontent.com/joelun37/Question-Answering/master/Documents/simplified-nq-train-for-content.json).\n",
    "\n",
    "Vous pouvez également visualiser une observation de la base initiale, en cliquant [ici] ()\n",
    "\n",
    "Nous utiliserons les données simplifiées par Google:\n",
    "\n",
    "- **simplified-nq-train.jsonl**: 17.45 Go\n",
    "- **simplified-nq-test.jsonl**: 18.8 Mo\n",
    "\n",
    "La base d'apprentissage étant assez volumineuse, nous utiliserons le Google Cloud Platform pour nos calculs.\n",
    "\n",
    "Ci-dessous, les nombres d'observations dans les bases fournies par Google:\n",
    "\n",
    "- **Base d'apprentissage:** 307 373 observations\n",
    "- **Base de validation (ou de développement):** 7 830 observations\n",
    "- **Base de test:** 7 842 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://aytuncs-mbp-001:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>QA</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11800d668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "local = \"local[*]\"\n",
    "appName = \"QA\"\n",
    "configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
    "set(\"spark.executor.memory\", \"6G\").\\\n",
    "set(\"spark.driver.memory\", \"6G\").\\\n",
    "set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'inspecter le format du fichier `json`nous avons créé une version courte, que nous chargeons ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_content = spark.read.json(\"/Volumes/750GB-HDD/root/Question-Answering/pyData/tensorflow2-question-answering/simplified-nq-train-for-content.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annotations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotation_id: long (nullable = true)\n",
      " |    |    |-- long_answer: struct (nullable = true)\n",
      " |    |    |    |-- candidate_index: long (nullable = true)\n",
      " |    |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- short_answers: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- yes_no_answer: string (nullable = true)\n",
      " |-- document_text: string (nullable = true)\n",
      " |-- document_url: string (nullable = true)\n",
      " |-- example_id: long (nullable = true)\n",
      " |-- long_answer_candidates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- top_level: boolean (nullable = true)\n",
      " |-- question_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for_content.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les variables\n",
    "\n",
    "### Annotations\n",
    "\n",
    "Cette variable contient réponses **longues** et **courtes**, s'il existe des réponses. Ci-dessous les règles de cette variable:\n",
    "\n",
    "- Chaque question a une réponse longue au maximum. En revanche, il peut y avoir plusieurs réponses courtes.\n",
    "- Les réponses courtes sont nécessairement contenues dans la réponse longue. Si la réponse est de type **Oui/Non**, alors `yes_no_answer` prend les valeurs `Yes` ou `No`. Par défaut, la valeur de cette variable est `None`.\n",
    "- Seulement 1% des réponses sont de type **Oui/Non**.\n",
    "\n",
    "### Document text\n",
    "Cette variable contient le corps de la page Wikipedia en format *html*.\n",
    "\n",
    "### Document URL\n",
    "Cette variable contient le lien URL vers la page Wikipedia.\n",
    "\n",
    "### Example ID\n",
    "C'est l'identifiant de l'exemple.\n",
    "\n",
    "### Long answer candidates\n",
    "Cette variable contient les réponses candidates. \n",
    "\n",
    "- Parfois, une longue réponse est imbriquée dans une autre.\n",
    "- Pour différencier ces deux types de réponses, on utilise la notion de **niveau**. Une réponse est donc contenue dans une autre si son indicateur *top level* est `False`.\n",
    "- 95% des réponses longues sont du *top level* `True`. Nous pourrions donc, dans un premier temps, nous focaliser sur ces réponses uniquement.\n",
    "\n",
    "### Question text\n",
    "C'est la question posée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'annotations'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_content.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_content_2 = spark.read.json(\"/Volumes/750GB-HDD/root/Question-Answering/pyData/tensorflow2-question-answering/v1.0-simplified_nq-dev-all-for-content.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annotations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotation_id: decimal(20,0) (nullable = true)\n",
      " |    |    |-- long_answer: struct (nullable = true)\n",
      " |    |    |    |-- candidate_index: long (nullable = true)\n",
      " |    |    |    |-- end_byte: long (nullable = true)\n",
      " |    |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |    |-- start_byte: long (nullable = true)\n",
      " |    |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- short_answers: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end_byte: long (nullable = true)\n",
      " |    |    |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |    |    |-- start_byte: long (nullable = true)\n",
      " |    |    |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- yes_no_answer: string (nullable = true)\n",
      " |-- document_html: string (nullable = true)\n",
      " |-- document_title: string (nullable = true)\n",
      " |-- document_tokens: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- end_byte: long (nullable = true)\n",
      " |    |    |-- html_token: boolean (nullable = true)\n",
      " |    |    |-- start_byte: long (nullable = true)\n",
      " |    |    |-- token: string (nullable = true)\n",
      " |-- document_url: string (nullable = true)\n",
      " |-- example_id: long (nullable = true)\n",
      " |-- long_answer_candidates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- end_byte: long (nullable = true)\n",
      " |    |    |-- end_token: long (nullable = true)\n",
      " |    |    |-- start_byte: long (nullable = true)\n",
      " |    |    |-- start_token: long (nullable = true)\n",
      " |    |    |-- top_level: boolean (nullable = true)\n",
      " |-- question_text: string (nullable = true)\n",
      " |-- question_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for_content_2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première tentative de modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: which is the most common use of opt-in e-mail marketing\n",
      "Answe found by the model: referral marketing\n",
      "\n",
      "Long Answer: <P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P> \n",
      "Short Answer: a newsletter sent to an advertising firm 's customers \n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Mock test\n",
    "# We are going to get 2 long answer candidates with the real long answer\n",
    "# and feed it to the BERT model\n",
    "\n",
    "example_txt = \"/Volumes/750GB-HDD/root/Question-Answering/pyData/tensorflow2-question-answering/simplified-nq-train-for-content.json\"\n",
    "dev_example = \"/Volumes/750GB-HDD/root/Question-Answering/pyData/tensorflow2-question-answering/v1.0-simplified_nq-dev-all-for-content.json\"\n",
    "\n",
    "def raw_NQ_data_dict(input_text_file):\n",
    "\n",
    "    with open(input_text_file, 'r') as f:\n",
    "        for line in f:\n",
    "            example_dict = json.loads(line) \n",
    "            simplfied_ex =   (example_dict)\n",
    "\n",
    "    return simplfied_ex\n",
    "\n",
    "test_dict = raw_NQ_data_dict(input_text_file=example_txt)\n",
    "\n",
    "long_answer_candidates = test_dict[\"long_answer_candidates\"]\n",
    "document_text = test_dict[\"document_text\"]\n",
    "question_text = test_dict[\"question_text\"]\n",
    "\n",
    "# [{'yes_no_answer': 'NONE',\n",
    "#   'long_answer': {'start_token': 1952,\n",
    "#    'candidate_index': 54,\n",
    "#    'end_token': 2019},\n",
    "#   'short_answers': [{'start_token': 1960, 'end_token': 1969}],\n",
    "#   'annotation_id': 593165450220027640}]\n",
    "annotations = test_dict[\"annotations\"]\n",
    "\n",
    "long_answer = \"\"\n",
    "candidate_index = annotations[0][\"long_answer\"][\"candidate_index\"]\n",
    "\n",
    "for i in range(long_answer_candidates[candidate_index][\"start_token\"], \\\n",
    "               long_answer_candidates[candidate_index][\"end_token\"]):\n",
    "    long_answer += document_text.split()[i] + \" \"\n",
    "\n",
    "short_answer = \"\"\n",
    "for i in range(annotations[0][\"short_answers\"][0][\"start_token\"], \\\n",
    "               annotations[0][\"short_answers\"][0][\"end_token\"]):\n",
    "    short_answer += document_text.split()[i] + \" \"\n",
    "\n",
    "# First two candidates\n",
    "candidate_dict = {}\n",
    "\n",
    "i = 0\n",
    "while len(candidate_dict.keys()) < 2:\n",
    "    if long_answer_candidates[i][\"top_level\"] == True:\n",
    "        txt = \"\"\n",
    "        for j in range(long_answer_candidates[i][\"start_token\"], \\\n",
    "                       long_answer_candidates[i][\"end_token\"]):\n",
    "            txt += document_text.split()[j] + \" \"\n",
    "        candidate_dict[i] = txt\n",
    "    i += 1\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "text = \"\"\n",
    "for key in candidate_dict.keys():\n",
    "    text += remove_html_tags(candidate_dict[key])\n",
    "\n",
    "inputs = tokenizer.encode_plus(question_text, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer_start_scores, answer_end_scores = model(**inputs)\n",
    "answer_start = torch.argmax(\n",
    "    answer_start_scores\n",
    ")  # Get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "print(f\"Question: {question_text}\\n\")\n",
    "print(f\"Answer found by the model: {answer}\\n\")\n",
    "print(f\"Long Answer: {remove_html_tags(long_answer)}\\n\")\n",
    "print(f\"Short Answer: {short_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
