{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human language and word meaning\n",
    "\n",
    "In NLP, We want to represent the meaning of words. Words have a lot of rich meaning. Question then becomes: *what is meaning?*\n",
    "\n",
    "### How do we have usable meaning in a computer?\n",
    "Common solution is using *WordNet*, a thesaurus containing lists of **synonym sets** and **hypernyms** (\"is a\" relationships)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.clos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun: good\n",
      "noun: good,goodness\n",
      "noun: good,goodness\n",
      "noun: commodity,trade_good,good\n",
      "adj: good\n",
      "adj (s): full,good\n",
      "adj: good\n",
      "adj (s): estimable,good,honorable,respectable\n",
      "adj (s): beneficial,good\n",
      "adj (s): good\n",
      "adj (s): good,just,upright\n",
      "adj (s): adept,expert,good,practiced,proficient,skillful,skilful\n",
      "adj (s): good\n",
      "adj (s): dear,good,near\n",
      "adj (s): dependable,good,safe,secure\n",
      "adj (s): good,right,ripe\n",
      "adj (s): good,well\n",
      "adj (s): effective,good,in_effect,in_force\n",
      "adj (s): good\n",
      "adj (s): good,serious\n",
      "adj (s): good,sound\n",
      "adj (s): good,salutary\n",
      "adj (s): good,honest\n",
      "adj (s): good,undecomposed,unspoiled,unspoilt\n",
      "adj (s): good\n",
      "adv: well,good\n",
      "adv: thoroughly,soundly,good\n"
     ]
    }
   ],
   "source": [
    "poses = { \"n\":\"noun\", \"v\":\"verb\", \"s\":\"adj (s)\", \"a\":\"adj\", \"r\":\"adv\"}\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(\"{}: {}\".format(poses[synset.pos()],\n",
    "                          \",\".join([l.name() for l in synset.lemmas()]\n",
    "                                   )\n",
    "                          )\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with resources like WordNet\n",
    "\n",
    "- Missing nuance, missing new meanings of words, subjective, requires human labor to create and adapt, can't compute word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words as discrete symbols\n",
    "\n",
    "In traditional NLP, we regard words as discrete symbols. Basically, presenting words in one-hot vectors. Problem is the size of the vectors, in a regular English dictionary there are about 250,000 words and easy ot get to a million with scientific, etc. words. So basically there are an infinite number of words in languages, in English by using derivational morphology you can create new words.\n",
    "\n",
    "There'a much bigger problem then that:lack of similarity relationship. Vectors are orthgonal. Google's solution in around 2005 was using Similarity matrices, that is a dictionary with similiarities like *hotel-motel*.\n",
    "\n",
    "### Representing words by their context\n",
    "\n",
    "It's called *distributional semantics*.\n",
    "\n",
    "\"You shall know a word by the company it keeps\" -J.R. Firth, 1957\n",
    "\n",
    "Using words that exist around one word in different sentences to describe it. It does a great job at capturing meaning.\n",
    "\n",
    "### Word vectors\n",
    "\n",
    "Somestimes it's called *word embeddings*, or *word representations*. It's a *distributed* representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec: Overview\n",
    "\n",
    "**Word2vec** (Mikolov et al. 2013) is a framework for learning word vectors.\n",
    "\n",
    "Idea:\n",
    "- We have a large corpus of text\n",
    "- Every word in a fixed vocabulary is represented by a **vector**\n",
    "- Go through each position *t* in the text, which has a center word *c* and context (\"outside\") words *o*\n",
    "- Use the **similarity of the word vectors** for *c* and *o* to **calculate the probability** of *o* given *c* (or vice versa)\n",
    "- **Keep adjusting the word vectors** to maximize this probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec: objective function\n",
    "\n",
    "#### Likelihood\n",
    "\n",
    "Basically, product of conditionatl probabilities over the whole corpus of words, corpus just becomes a long list of words.\n",
    "\n",
    "Parameter: we're representing a word by a vector in the vector space and that representation is it's meaning and we're going to use that to predict what other words occur.\n",
    "\n",
    "\n",
    "#### Objective function / Loss function\n",
    "\n",
    "Basically, just the negative of the log of the likelihood. Log will allow us to sum the log of the probability.\n",
    "\n",
    "#### How to calculate the probability of a word at *t+j* given word at *t*?\n",
    "\n",
    "We will use *two vectors* per word *w*:\n",
    "- $v_w$ when *w* is a center word\n",
    "- $u_w$ when *w* is a context word\n",
    "\n",
    "We write $P(u_{problems} \\mid v_{into})$ short for $P(problems \\mid into;u_{problems}, v_{into}, \\theta)$\n",
    "\n",
    "#### Word2vec: prediction function\n",
    "\n",
    "Dot product is our similarity measure.\n",
    "\n",
    "Exponentiated dot product of two vectors: center and outside. Larger dot product = latger probability. Exponentiation makes anything positive.\n",
    "\n",
    "Then normalize over entire vocabulary by dividing the exponention by the sum of all probabilities.\n",
    "\n",
    "Softmax always turn any numbers into a probability distribution. \n",
    "\n",
    "#### Training a model by optimizing parameters\n",
    "\n",
    "We have a loss function with a probability model on the inside that we can build and so what we want to be able to do is then move our vector representations of words around so that they are good at predicting what words occur in the context of other words. That is, optimization.\n",
    "\n",
    "To train a model, we adjust parameters to minimize a loss, e.g.for a simple convex function over two parameters, contour lines show levels of objective function.\n",
    "\n",
    "#### Compute all vector gradients\n",
    "\n",
    "We want to make a very big vector in a very high-dimensional vector space of all the parameters of our model and the only parameters that his model has is literally the vector space representations of words. So if there are a 100-dimensional word representations, there are 100 parameters for \"aardvark\" in context, 100 parameters for the word \"art\" in **context**, etc. and then 100 parameters for the word \"aardvark\" as a **center word**, etc.\n",
    "\n",
    "That gives a big vector of parameters to optimize.\n",
    "\n",
    "- Recall: $\\theta$ represents **all** model parameters in one long vector\n",
    "- In our case, with *d*-dimensional vectors and *V*-many words\n",
    "- Remember: every word has two vectors\n",
    "- We optimize these parameters by walking down the gradient\n",
    "\n",
    "#### Concrete example\n",
    "\n",
    "The way we calculate the $u$ and $v$ vectors is we're literally going to start with a random vector for each word and then iteratively going to change those vectors a little bit as we learn. And the way we're going to work out how we change them is the following.\n",
    "\n",
    "We're going to say, \"I want to do optimization. Okay, we have the current vectors for each word. Let me do some calculus to work out how I could change the word vectors to mean that the word vectors would calculate a higher probability for the words that actually occur in the context of this center word.\" And we will do that again and again and again. \n",
    "\n",
    "Minimize $J(\\theta)$ by changing the parameters $\\theta$, where $\\theta$ are the the vectors $u$ and $v$.\n",
    "\n",
    "What I want to achieve for my distributional notion of meaning is, I have a meaningful word, a vector and that knows what words occur in the context of a word-itself. And knowing what words occur in its context means, it can accurately give a high probability estimate to those words that occur in the context and it will give low probability to words that don't typically occur in the context. So, if the word is \"bank\", I'm hoping that words like \"branch\", \"open\", \"withdrawal\" will be given high probability because they tend to occur with the word \"bank\".\n",
    "\n",
    "Obviously, we're not going to be able to do this super-well or we're just not gonna be able to say \"Oh, the word in the context is going to be is this word with probability .97\" because we're using one simple probability distribution to predict all words in our context. So, it's going to be a very loose model.\n",
    "\n",
    "**Conclusion, after taking partial derivates:**\n",
    "\n",
    "The partial derivative of the logarithm of $\\Pr(o \\mid c)$ is our slope in the multi-dimensional space and how we're getting that slope is:\n",
    "\n",
    "- we're taking the observed representation $u_o$ of the context word and\n",
    "- we're subtracting from that what our model thinks what the context should look like.\n",
    "\n",
    "What does the model think that the context should like?\n",
    "The sum part is formal in expectation. Basically we calculate a weighted average by multiplying the models of the representations of word with the probability of the word in the current model.\n",
    "\n",
    "So, we are calculating the difference between the actual context word and the expected context word, that difference turns out to exactly give the slope as to which direction we should be walking, changing the words representation in order to improve our model's ability to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim word vector visualization of various word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For looking at word vectors, we'll use Gensim. We also use it in hw1 for word vectors. Gensim isn't really a deep learning package. It's a package for word and text similarity modeling, which started with LDA-style topic models and grew into SVD and neural word representations. But it is efficient and scalable, and quite widely used.\n",
    "\n",
    "Our homegrown Stanford offering is GloVe word vectors. Gensim doesn't give them first class support, but allows you to convert a file of GloVe vectors into word2vec format. You can download the GloVe vectors from [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)\n",
    "\n",
    "(I use the 100d vectors below as a mix between speed and smallness vs. quality. If you try out the 50d vectors, they basically work for similarity but clearly aren't as good for analogy problems. If you load the 300d vectors, they're even better than the 100d vectors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/Volumes/750GB-HDD/root/\"\n",
    "data_folder = \"Question-Answering/pyData/\"\n",
    "glove_file = datapath(root + data_folder + 'glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('superman', 0.8058773279190063),\n",
       " ('superhero', 0.6820072531700134),\n",
       " ('sequel', 0.6592288613319397),\n",
       " ('catwoman', 0.6541578769683838),\n",
       " ('joker', 0.6362104415893555),\n",
       " ('comics', 0.6360483765602112),\n",
       " ('marvel', 0.6221269369125366),\n",
       " ('spider-man', 0.6080650687217712),\n",
       " ('villain', 0.6072646379470825),\n",
       " ('gotham', 0.5963695049285889)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"batman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyrates', 0.7173939347267151),\n",
       " ('sungrebe', 0.7119239568710327),\n",
       " ('þórður', 0.7067720890045166),\n",
       " ('zety', 0.7056615352630615),\n",
       " ('23aou94', 0.6959497928619385),\n",
       " ('___________________________________________________________',\n",
       "  0.6949152946472168),\n",
       " ('elymians', 0.6945434212684631),\n",
       " ('camarina', 0.6927202939987183),\n",
       " ('ryryryryryry', 0.6905654072761536),\n",
       " ('maurilio', 0.6865653395652771)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most dissimilar\n",
    "model.most_similar(negative=\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'washington'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"france\", \"paris\", \"united\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/750GB-HDD/root/Question-Answering/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
